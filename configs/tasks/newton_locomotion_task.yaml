task_name: "newton_locomotion"
device: "cuda"
policy: "MlpPolicy"
seed: 75129

n_envs: 4096
timesteps: 25_000
episode_length: 2000 # 20 seconds of real time (2000 * 0.01 = 20)
reset_in_play: False

delay:
  enabled: False
  obs_delay_range: [1, 8] # sample range of time steps to delay the observations by
  act_delay_range: [1, 2] # sample range of time steps to delay the actions by
  instant_rewards: False # should the rewards be calculated on the brain or remote actor?

scalers:
  observations:
    projected_gravities: 1.0
    linear_velocities: 2.0
    angular_velocities: 0.25
    joint_positions: 1.0
    joint_velocities: 0.05
    last_actions: 1.0
    velocity_commands: 1.0

  rewards:
    position: 0.5
    height: 2.0
    base_stability: 2.0
    base_linear_velocity_xy: 1.0
    base_linear_velocity_z: 1.0
    base_angular_velocity_z: 0.5
    joint_positions: 2.0
    joint_action_rate: 0.05
    air_time: 0.5
    survival: 5.0

  action: 1.0

  commands:
    linear_velocity_xy: 1.0
    angular_velocity_z: 1.0

ppo:
  n_steps: 24 # 0.48 seconds of real time (24 * 0.02 = 0.48)
  n_epochs: 5
  n_minibatches: 8

  discount_factor: 0.99
  lambda: 0.95

  learning_rate: 3e-4
  learning_rate_scheduler: "KLAdaptiveRL"
  learning_rate_scheduler_kwargs:
    kl_threshold: 0.008

  grad_norm_clip: 1.0
  ratio_clip: 0.2
  value_clip: 0.2
  clip_predicted_values: False

  entropy_loss_coefficient: 0.0
  value_loss_coefficient: 1.0

  stopping_kl_threshold: 0.0

  mixed_precision: True

  state_preprocessor: "RunningStandardScaler"
  state_preprocessor_kwargs: None

  value_preprocessor: "RunningStandardScaler"
  value_preprocessor_kwargs: None

  experiment:
    write_interval: "auto"
    checkpoint_interval: "auto"
